apiVersion: llm.geeper.io/v1alpha1
kind: LMDeployment
metadata:
  name: openwebui-with-pipelines
  namespace: default
  labels:
    app: openwebui-with-pipelines
    component: ai-chat-with-pipelines
spec:
  ollama:
    models:
      - "gemma3:270m"
  openwebui:
    enabled: true
    replicas: 1
    image: ghcr.io/open-webui/open-webui:main
    ingress:
      host: openwebui.localhost
    # Enable pipelines integration
    pipelines:
      enabled: true
      image: ghcr.io/open-webui/pipelines:main
      port: 9099
      replicas: 1
      serviceType: ClusterIP
      pipelinesDir: "/app/pipelines"
      persistence:
        enabled: true
        size: "5Gi"
    langfuse:
      enabled: true
      debug: false
      projectName: "openwebui-pipelines-project"
      environment: "development"
    service:
      type: ClusterIP
      port: 8080
    resources:
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"

