apiVersion: llm.geeper.io/v1alpha1
kind: LMDeployment
metadata:
  name: openwebui-langfuse-only
  namespace: default
  labels:
    app: openwebui-langfuse-only
    component: ai-chat-with-monitoring
spec:
  ollama:
    models:
      - "llama2:7b"

    service:
      type: ClusterIP
      port: 11434
  
  openwebui:
    enabled: true
    replicas: 1
    image: ghcr.io/open-webui/open-webui:main

    service:
      type: ClusterIP
      port: 8080
    ingress:
      enabled: true
      host: "chat-with-monitoring.localhost"
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        nginx.ingress.kubernetes.io/ssl-redirect: "false"
    
    # Langfuse monitoring configuration (no pipelines required)
    langfuse:
      enabled: true
      url: "https://cloud.langfuse.com"
      publicKey: "your-langfuse-public-key"
      secretKey: "your-langfuse-secret-key"
      projectName: "simple-ai-chat"
      environment: "development"
      debug: true
    
    # Optional: Enable pipelines to get automatic Langfuse monitoring pipeline
    # pipelines:
    #   enabled: true
    #   image: ghcr.io/open-webui/pipelines:main
    #   # Note: Langfuse monitoring pipeline is automatically added when langfuse.enabled=true
    
    # Redis for session management
    redis:
      enabled: true
      image: redis:7-alpine

      persistence:
        enabled: true
        size: "5Gi"
