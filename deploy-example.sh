#!/bin/bash

# Deploy Example Script for LLM Operator
# This script demonstrates how to deploy and use the OllamaDeployment CRD

set -e

echo "ğŸš€ Deploying LLM Operator with OllamaDeployment CRD..."

# Build and deploy the operator
echo "ğŸ“¦ Building operator..."
make build

echo "ğŸ”§ Installing CRDs..."
make install

echo "ğŸš€ Deploying operator..."
make deploy

echo "â³ Waiting for operator to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/llm-operator-controller-manager -n llm-operator-system

echo "âœ… Operator deployed successfully!"

# Create a namespace for our example
echo "ğŸ—ï¸ Creating example namespace..."
kubectl create namespace ollama-example --dry-run=client -o yaml | kubectl apply -f -

# Deploy the example OllamaDeployment
echo "ğŸ“‹ Deploying example OllamaDeployment..."
kubectl apply -f config/samples/v1alpha1_ollama_deployment.yaml

echo "â³ Waiting for OllamaDeployment to be ready..."
kubectl wait --for=condition=ready --timeout=600s ollamadeployment/ollama-example -n default

echo "ğŸ‰ Deployment complete!"
echo ""
echo "ğŸ“Š Check the status:"
echo "kubectl get ollamadeployment ollama-example -o yaml"
echo ""
echo "ğŸ” View created resources:"
echo "kubectl get all -l ollama-deployment=ollama-example"
echo ""
echo "ğŸŒ Access OpenWebUI (if ingress is configured):"
echo "Add 'ollama-webui.local' to your /etc/hosts file pointing to your cluster IP"
echo ""
echo "ğŸ§¹ To clean up:"
echo "kubectl delete -f config/samples/v1alpha1_ollama_deployment.yaml"
echo "make undeploy"
echo "make uninstall"
